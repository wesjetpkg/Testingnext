import * as tracing_1 from "@effect-ts/core/Tracing";
const fileName_1 = "packages/@contentlayer/core/src/generation/generate-dotpkg.ts";
import { filePathJoin } from '@contentlayer/utils';
import * as utils from '@contentlayer/utils';
import { Array, Chunk, flow, OT, pipe, S, T } from '@contentlayer/utils/effect';
import { fs } from '@contentlayer/utils/node';
import { camelCase } from 'camel-case';
import { ArtifactsDir } from '../ArtifactsDir.js';
import { autogeneratedNote } from './common.js';
import { renderTypes } from './generate-types.js';
export const logGenerateInfo = (info) => T.log(`Generated ${info.documentCount} documents in .contentlayer`);
export const generateDotpkg = ({ config, verbose, }) => (OT.withSpan('@contentlayer/core/generation:generateDotpkg', { attributes: { verbose } })(T.rightOrFail(T.map_(S.runCollect(S.take_(generateDotpkgStream({ config, verbose, isDev: false }), 1)), Chunk.unsafeHead, fileName_1 + ":60:10"), fileName_1 + ":61:18")));
// TODO make sure unused old generated files are removed
export const generateDotpkgStream = ({ config, verbose, isDev, }) => {
    const writtenFilesCache = {};
    const generationOptions = { sourcePluginType: config.source.type, options: config.source.options };
    const resolveParams = (T.either(T.structPar({
        schemaDef: config.source.provideSchema(config.esbuildHash),
        targetPath: ArtifactsDir.mkdir,
    }, fileName_1 + ":78:16"), fileName_1 + ":82:13"));
    // .pipe(
    //   tap((artifactsDir) => watchData && errorIfArtifactsDirIsDeleted({ artifactsDir }))
    // ),
    return (S.chainMapEitherRight(({ schemaDef, targetPath }) => (S.mapEffectEitherRight((cache) => (T.eitherMap(() => ({ documentCount: Object.keys(cache.cacheItemsMap).length }))(writeFilesForCache({ schemaDef, targetPath, cache, generationOptions, writtenFilesCache, isDev }), fileName_1 + ":97:92")))(config.source.fetchData({ schemaDef, verbose }))))(S.fromEffect(resolveParams)));
};
const writeFilesForCache = ({ cache, schemaDef, targetPath, generationOptions, writtenFilesCache, isDev, }) => (T.either(OT.withSpan('@contentlayer/core/generation/generate-dotpkg:writeFilesForCache', {
    attributes: {
        targetPath,
        cacheKeys: Object.keys(cache.cacheItemsMap),
    },
})(T.gen(function* ($) {
    const withPrefix = (...path_) => filePathJoin(targetPath, ...path_);
    if (process.env['CL_DEBUG']) {
        yield* $(fs.mkdirp(withPrefix('.cache')), fileName_1 + ":129:17");
        yield* $(T.collectAllPar([
            fs.writeFileJson({ filePath: withPrefix('.cache', 'schema.json'), content: schemaDef }),
            fs.writeFileJson({ filePath: withPrefix('.cache', 'data-cache.json'), content: cache }),
        ], fileName_1 + ":131:26"), fileName_1 + ":130:17");
    }
    const allCacheItems = Object.values(cache.cacheItemsMap);
    const allDocuments = allCacheItems.map((_) => _.document);
    const documentDefs = Object.values(schemaDef.documentTypeDefMap);
    const [nodeVersionMajor, nodeVersionMinor] = yield* $(T.succeedWith(() => process.versions.node.split('.').map((_) => parseInt(_, 10)), fileName_1 + ":144:22"), fileName_1 + ":143:60");
    // NOTE Type assert statements for `.json` files are neccessary from Node v16.14 onwards
    const needsJsonAssertStatement = nodeVersionMajor > 16 || (nodeVersionMajor === 16 && nodeVersionMinor >= 14);
    const assertStatement = needsJsonAssertStatement ? ` assert { type: 'json' }` : '';
    const typeNameField = generationOptions.options.fieldOptions.typeFieldName;
    const dataBarrelFiles = documentDefs.map((docDef) => ({
        content: makeDataExportFile({
            docDef,
            documentIds: allDocuments.filter((_) => _[typeNameField] === docDef.name).map((_) => _._id),
            assertStatement,
        }),
        filePath: withPrefix('generated', docDef.name, `_index.mjs`),
    }));
    const individualDataJsonFiles = allCacheItems.map(({ document, documentHash }) => ({
        content: JSON.stringify(document, null, 2),
        filePath: withPrefix('generated', document[typeNameField], `${idToFileName(document._id)}.json`),
        documentHash,
    }));
    const collectionDataJsonFiles = (Array.map_(documentDefs, (documentDef) => {
        const documents = allDocuments.filter((_) => _[typeNameField] === documentDef.name);
        const jsonData = documentDef.isSingleton ? documents[0] : documents;
        return {
            content: JSON.stringify(jsonData, null, 2),
            filePath: withPrefix('generated', documentDef.name, `_index.json`),
            documentHash: documents.map((_) => _.documentHash).join(''),
        };
    }));
    const dataDirPaths = documentDefs.map((_) => withPrefix('generated', _.name));
    yield* $(T.forEachPar_([withPrefix('generated'), ...dataDirPaths], fs.mkdirp, fileName_1 + ":182:29"), fileName_1 + ":182:15");
    const writeFile = writeFileWithWrittenFilesCache({ writtenFilesCache });
    yield* $(T.collectAllPar([
        writeFile({ filePath: withPrefix('package.json'), content: makePackageJson(schemaDef.hash) }),
        writeFile({
            filePath: withPrefix('generated', 'types.d.ts'),
            content: renderTypes({ schemaDef, generationOptions }),
            rmBeforeWrite: true,
        }),
        writeFile({
            filePath: withPrefix('generated', 'index.d.ts'),
            content: makeDataTypes({ schemaDef }),
            rmBeforeWrite: true,
        }),
        writeFile({
            filePath: withPrefix('generated', 'index.mjs'),
            content: makeIndexMjs({ schemaDef, assertStatement, isDev }),
        }),
        ...dataBarrelFiles.map(writeFile),
        ...individualDataJsonFiles.map(writeFile),
        ...collectionDataJsonFiles.map(writeFile),
        // TODO generate readme file
    ], fileName_1 + ":187:24"), fileName_1 + ":186:15");
}, fileName_1 + ":125:10")), fileName_1 + ":216:13"));
const makePackageJson = (schemaHash) => {
    const packageJson = {
        name: 'dot-contentlayer',
        description: 'This package is auto-generated by Contentlayer',
        // TODO generate more meaningful version (e.g. by using Contentlayer version and schema hash)
        version: `0.0.0-${schemaHash}`,
        exports: {
            './generated': {
                import: './generated/index.mjs',
            },
        },
        typesVersions: {
            '*': {
                generated: ['./generated'],
            },
        },
    };
    return JSON.stringify(packageJson, null, 2);
};
/**
 * Remembers which files already have been written to disk.
 * If no `documentHash` was provided, the writes won't be cached.
 *
 * TODO maybe rewrite with effect-cache
 */
const writeFileWithWrittenFilesCache = ({ writtenFilesCache }) => ({ filePath, content, documentHash, rmBeforeWrite = true, }) => T.gen(function* ($) {
    // TODO also consider schema hash
    const fileIsUpToDate = documentHash !== undefined && writtenFilesCache[filePath] === documentHash;
    if (!rmBeforeWrite && fileIsUpToDate) {
        return;
    }
    if (rmBeforeWrite) {
        yield* $(fs.rm(filePath, { force: true }), fileName_1 + ":268:17");
    }
    yield* $(fs.writeFile(filePath, content), fileName_1 + ":270:15");
    if (documentHash) {
        writtenFilesCache[filePath] = documentHash;
    }
}, fileName_1 + ":260:10");
const makeDataExportFile = ({ docDef, documentIds, assertStatement, }) => {
    const dataVariableName = getDataVariableName({ docDef });
    if (docDef.isSingleton) {
        const documentId = documentIds[0];
        return `\
// ${autogeneratedNote}
export { default as ${dataVariableName} } from './${idToFileName(documentId)}.json'${assertStatement}
`;
    }
    const makeVariableName = flow(idToFileName, (_) => camelCase(_, { stripRegexp: /[^A-Z0-9\_]/gi }));
    const docImports = documentIds
        .map((_) => `import ${makeVariableName(_)} from './${idToFileName(_)}.json'${assertStatement}`)
        .join('\n');
    return `\
// ${autogeneratedNote}

${docImports}

export const ${dataVariableName} = [${documentIds.map((_) => makeVariableName(_)).join(', ')}]
`;
};
const makeIndexMjs = ({ schemaDef, assertStatement, isDev, }) => {
    const dataVariableNames = Object.values(schemaDef.documentTypeDefMap).map((docDef) => ({
        isSingleton: docDef.isSingleton,
        documentDefName: docDef.name,
        dataVariableName: getDataVariableName({ docDef }),
    }));
    const constExports = 'export { ' + dataVariableNames.map((_) => _.dataVariableName).join(', ') + ' }';
    const constImportsForAllDocuments = dataVariableNames
        .map(({ documentDefName, dataVariableName }) => isDev
        ? `import { ${dataVariableName} } from './${documentDefName}/_index.mjs'`
        : `import ${dataVariableName} from './${documentDefName}/_index.json'${assertStatement}`)
        .join('\n');
    const allDocuments = dataVariableNames
        .map(({ isSingleton, dataVariableName }) => (isSingleton ? dataVariableName : `...${dataVariableName}`))
        .join(', ');
    return `\
// ${autogeneratedNote}

export { isType } from 'contentlayer/client'

// NOTE During development Contentlayer imports from \`.mjs\` files to improve HMR speeds.
// During (production) builds Contentlayer it imports from \`.json\` files to improve build performance.
${constImportsForAllDocuments}

${constExports}

export const allDocuments = [${allDocuments}]
`;
};
export const makeDataTypes = ({ schemaDef }) => {
    const dataConsts = Object.values(schemaDef.documentTypeDefMap)
        .map((docDef) => [docDef, docDef.name, getDataVariableName({ docDef })])
        .map(([docDef, typeName, dataVariableName]) => `export declare const ${dataVariableName}: ${typeName}${docDef.isSingleton ? '' : '[]'}`)
        .join('\n');
    const documentTypeNames = Object.values(schemaDef.documentTypeDefMap)
        .map((docDef) => docDef.name)
        .join(', ');
    return `\
// ${autogeneratedNote}

import { ${documentTypeNames}, DocumentTypes } from './types'

export * from './types'

${dataConsts}

export declare const allDocuments: DocumentTypes[]

`;
};
const getDataVariableName = ({ docDef }) => {
    if (docDef.isSingleton) {
        return utils.lowercaseFirstChar(utils.inflection.singularize(docDef.name));
    }
    else {
        return 'all' + utils.uppercaseFirstChar(utils.inflection.pluralize(docDef.name));
    }
};
const idToFileName = (id) => leftPadWithUnderscoreIfStartsWithNumber(id).replace(/\//g, '__');
const leftPadWithUnderscoreIfStartsWithNumber = (str) => {
    if (/^[0-9]/.test(str)) {
        return '_' + str;
    }
    return str;
};
//# sourceMappingURL=generate-dotpkg.js.map